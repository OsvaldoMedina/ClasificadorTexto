{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6YTaQq9dCh0E"
   },
   "source": [
    "# Clasificación de palabras (por género de nombre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "id": "5co_TuOhC4ze",
    "outputId": "6ed198ee-9cb9-48b9-ab74-db9655585c0e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     C:\\Users\\osval\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk, random\n",
    "nltk.download('names')\n",
    "from nltk.corpus import names "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijhsE2PBFYxm"
   },
   "source": [
    "**Función básica de extracción de atributos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "b0kKV62lCZ55"
   },
   "outputs": [],
   "source": [
    "# definición de atributos relevantes\n",
    "def atributos(palabra):\n",
    "\treturn {'ultima_letra': palabra[-1]}\n",
    "\n",
    "tagset = ([(name, 'male') for name in names.words('male.txt')] + [(name, 'female') for name in names.words('female.txt')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "id": "IjfK5ZKwDL__",
    "outputId": "fef1000d-e474-43f0-986d-10b721946e59"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Aamir', 'male'),\n",
       " ('Aaron', 'male'),\n",
       " ('Abbey', 'male'),\n",
       " ('Abbie', 'male'),\n",
       " ('Abbot', 'male'),\n",
       " ('Abbott', 'male'),\n",
       " ('Abby', 'male'),\n",
       " ('Abdel', 'male'),\n",
       " ('Abdul', 'male'),\n",
       " ('Abdulkarim', 'male')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "id": "jZcAN-dmCrok",
    "outputId": "b114852f-8e71-4121-fda3-21bb731353b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Christoph', 'male'),\n",
       " ('Korry', 'female'),\n",
       " ('Jacob', 'male'),\n",
       " ('Mavra', 'female'),\n",
       " ('Abdul', 'male'),\n",
       " ('Adora', 'female'),\n",
       " ('Elora', 'female'),\n",
       " ('Richmond', 'male'),\n",
       " ('Pattie', 'male'),\n",
       " ('Heida', 'female')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.shuffle(tagset)\n",
    "tagset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "QzK97C8BDmHR"
   },
   "outputs": [],
   "source": [
    "fset = [(atributos(n), g) for (n, g) in tagset]\n",
    "train, test = fset[500:], fset[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQPv0tW4Fd2G"
   },
   "source": [
    "**Modelo de clasificación Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "37jueg4nDQFs"
   },
   "outputs": [],
   "source": [
    "# entrenamiento del modelo NaiveBayes\n",
    "classifier = nltk.NaiveBayesClassifier.train(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BAWfUSHrEj3q"
   },
   "source": [
    " **Verificación de algunas predicciones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Mr8ytm8SEEZk",
    "outputId": "cf62ff8a-2722-4331-bf70-66aafff1d9e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'female'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(atributos('amanda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "c0GG1Y1_EPaO",
    "outputId": "6f286792-7845-44f8-b22b-1cf6815036a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'male'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(atributos('peter'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XSUK14XhEqLL"
   },
   "source": [
    "**Performance del modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 32
    },
    "id": "lenwC5agEdvT",
    "outputId": "1ceb5e52-4db6-4c5f-c714-dbf72f90e652"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.774\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(classifier, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 32
    },
    "id": "p5S9qeCgsJSg",
    "outputId": "6339e65d-d66e-4c96-9053-ea70d0abdea7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7620902740462118\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(classifier, train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSNI7OFxGib0"
   },
   "source": [
    "**Mejores atributos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "k5uaIAdDGlq8"
   },
   "outputs": [],
   "source": [
    "def mas_atributos(nombre):\n",
    "    atrib = {}\n",
    "    atrib[\"primera_letra\"] = nombre[0].lower()\n",
    "    atrib[\"ultima_letra\"] = nombre[-1].lower()\n",
    "    for letra in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        atrib[\"count({})\".format(letra)] = nombre.lower().count(letra)\n",
    "        atrib[\"has({})\".format(letra)] = (letra in nombre.lower())\n",
    "    return atrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6-gJIxKcHKvI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'primera_letra': 'j',\n",
       " 'ultima_letra': 'n',\n",
       " 'count(a)': 0,\n",
       " 'has(a)': False,\n",
       " 'count(b)': 0,\n",
       " 'has(b)': False,\n",
       " 'count(c)': 0,\n",
       " 'has(c)': False,\n",
       " 'count(d)': 0,\n",
       " 'has(d)': False,\n",
       " 'count(e)': 0,\n",
       " 'has(e)': False,\n",
       " 'count(f)': 0,\n",
       " 'has(f)': False,\n",
       " 'count(g)': 0,\n",
       " 'has(g)': False,\n",
       " 'count(h)': 1,\n",
       " 'has(h)': True,\n",
       " 'count(i)': 0,\n",
       " 'has(i)': False,\n",
       " 'count(j)': 1,\n",
       " 'has(j)': True,\n",
       " 'count(k)': 0,\n",
       " 'has(k)': False,\n",
       " 'count(l)': 0,\n",
       " 'has(l)': False,\n",
       " 'count(m)': 0,\n",
       " 'has(m)': False,\n",
       " 'count(n)': 1,\n",
       " 'has(n)': True,\n",
       " 'count(o)': 1,\n",
       " 'has(o)': True,\n",
       " 'count(p)': 0,\n",
       " 'has(p)': False,\n",
       " 'count(q)': 0,\n",
       " 'has(q)': False,\n",
       " 'count(r)': 0,\n",
       " 'has(r)': False,\n",
       " 'count(s)': 0,\n",
       " 'has(s)': False,\n",
       " 'count(t)': 0,\n",
       " 'has(t)': False,\n",
       " 'count(u)': 0,\n",
       " 'has(u)': False,\n",
       " 'count(v)': 0,\n",
       " 'has(v)': False,\n",
       " 'count(w)': 0,\n",
       " 'has(w)': False,\n",
       " 'count(x)': 0,\n",
       " 'has(x)': False,\n",
       " 'count(y)': 0,\n",
       " 'has(y)': False,\n",
       " 'count(z)': 0,\n",
       " 'has(z)': False}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mas_atributos('jhon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "FBu25HHgHQtK"
   },
   "outputs": [],
   "source": [
    "fset = [(mas_atributos(n), g) for (n, g) in tagset]\n",
    "train, test = fset[500:], fset[:500]\n",
    "classifier2 = nltk.NaiveBayesClassifier.train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 32
    },
    "id": "8hWR9hOzHlNe",
    "outputId": "97d5f514-dfd5-474a-8755-4127a7bcb8dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.756\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(classifier2, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rT7mmVnvFAcm"
   },
   "source": [
    "### Ejercicio de práctica\n",
    "\n",
    "**Objetivo:** Construye un classificador de nombres en español usando el siguiente dataset: \n",
    "https://github.com/jvalhondo/spanish-names-surnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NphJqahkFnjO"
   },
   "source": [
    "1. **Preparación de los datos**: con un `git clone` puedes traer el dataset indicado a tu directorio en Colab, luego asegurate de darle el formato adecuado a los datos y sus features para que tenga la misma estructura del ejemplo anterior con el dataset `names` de nombres en ingles. \n",
    "\n",
    "* **Piensa y analiza**: ¿los features en ingles aplican de la misma manera para los nombres en español?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# escribe tu código aquí\n",
    "!git clone https://github.com/jvalhondo/spanish-names-surnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_names = pd.read_csv('C:/Users/osval/Documents/Clasificacion_Texto/spanish-names-surnames/male_names.csv')\n",
    "female_names =  pd.read_csv('C:/Users/osval/Documents/Clasificacion_Texto/spanish-names-surnames/female_names.csv')\n",
    "female_names.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagnames = [(name.lower(), 'male') for name in male_names['name']] + [(name.lower(), 'female') for name in female_names['name']] \n",
    "random.shuffle(tagset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definición de atributos relevantes\n",
    "def atributos(palabra):\n",
    "    return {'ultima_letra': palabra[-1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando dataset con atributos\n",
    "fset = [(atributos(n), g) for (n, g) in tagset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OhbOT6_zFGHM"
   },
   "outputs": [],
   "source": [
    "# escribe tu código aquí\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KuY1Ux30F9uZ"
   },
   "source": [
    "2. **Entrenamiento y performance del modelo**: usando el classificador de Naive Bayes de NLTK entrena un modelo sencillo usando el mismo feature de la última letra del nombre, prueba algunas predicciones y calcula el performance del modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividiendo en train_data y test_data\n",
    "train_data, test_data = train_test_split(fset, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluacion\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data accuracy: 0.7564505978602895\n",
      "train_data accuracy 0.7644374508261211\n"
     ]
    }
   ],
   "source": [
    "print(f'test_data accuracy: {nltk.classify.accuracy(classifier, test_data)}')\n",
    "print(f'train_data accuracy {nltk.classify.accuracy(classifier, train_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "CvHwHTS8GT9I"
   },
   "outputs": [],
   "source": [
    "# escribe tu código aquí\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4a2jv85GXA_"
   },
   "source": [
    "3. **Mejores atributos:** Define una función como `atributos2()` donde puedas extraer mejores atributos con los cuales entrenar una mejor version del clasificador. Haz un segundo entrenamiento y verifica como mejora el performance de tu modelo. ¿Se te ocurren mejores maneras de definir atributos para esta tarea particular?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atributos2(nombre):\n",
    "    atrib = {}\n",
    "    atrib[\"nombre\"] = nombre.lower()\n",
    "    atrib[\"primera_letra\"] = nombre[0].lower()\n",
    "    atrib[\"ultima_letra\"] = nombre[-1].lower()\n",
    "    atrib[\"primeras_dos_letras\"] = nombre[:2].lower()\n",
    "    atrib[\"ultimas_dos_letras\"] = nombre[-2:].lower()\n",
    "\n",
    "    for letra in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        #atrib 3. numero de veces aparece la letra\n",
    "        atrib[\"count({})\".format(letra)] = nombre.lower().count(letra)\n",
    "        #atrib 4. si tiene o no la letra\n",
    "        atrib[\"has({})\".format(letra)] = (letra in nombre.lower())\n",
    "    return atrib\n",
    "\n",
    "fset2 = [(mas_atributos(n), g) for (n, g) in tagset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividiendo en train_data y test_data\n",
    "train_data2, test_data2 = train_test_split(fset2, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data accuracy: 0.7804878048780488\n",
      "test_data accuracy 0.7784770295783512\n"
     ]
    }
   ],
   "source": [
    "# Evaluacion\n",
    "classifier2 = nltk.NaiveBayesClassifier.train(train_data2)\n",
    "print(f'train_data accuracy: {nltk.classify.accuracy(classifier2, train_data2)}')\n",
    "print(f'test_data accuracy {nltk.classify.accuracy(classifier2, test_data2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "9G-E5_CXIQiO"
   },
   "outputs": [],
   "source": [
    "# escribe tu código aquí\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7CXFyfoGf4s"
   },
   "source": [
    "# Clasificación de documentos (email spam o no spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 32
    },
    "id": "Qfli08sgIzl_",
    "outputId": "7b634f01-0956-4843-8eb3-a4e5d2c77d3d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'datasets' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/pachocamacho1990/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 115
    },
    "id": "bHFKXxclJ5LC",
    "outputId": "e2878b62-45ed-482a-faa2-92ce90b87731"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\osval\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\osval\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "id": "33oKcvcjKrlM",
    "outputId": "6183550d-66c9-41a4-e2a6-52c9da7ec461"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clase</th>\n",
       "      <th>contenido</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>&lt;!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Tr...</td>\n",
       "      <td>[&lt;, !, DOCTYPE, HTML, PUBLIC, ``, -//W3C//DTD,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>&gt; Russell Turpin:\\r\\n&gt; &gt; That depends on how t...</td>\n",
       "      <td>[&gt;, Russell, Turpin, :, &gt;, &gt;, That, depends, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>Help wanted.  We are a 14 year old fortune 500...</td>\n",
       "      <td>[Help, wanted, ., We, are, a, 14, year, old, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>Request A Free No Obligation Consultation!\\r\\n...</td>\n",
       "      <td>[Request, A, Free, No, Obligation, Consultatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Is there a way to look for a particular file o...</td>\n",
       "      <td>[Is, there, a, way, to, look, for, a, particul...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   clase                                          contenido  \\\n",
       "0     -1  <!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Tr...   \n",
       "1      1  > Russell Turpin:\\r\\n> > That depends on how t...   \n",
       "2     -1  Help wanted.  We are a 14 year old fortune 500...   \n",
       "3     -1  Request A Free No Obligation Consultation!\\r\\n...   \n",
       "4      1  Is there a way to look for a particular file o...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [<, !, DOCTYPE, HTML, PUBLIC, ``, -//W3C//DTD,...  \n",
       "1  [>, Russell, Turpin, :, >, >, That, depends, o...  \n",
       "2  [Help, wanted, ., We, are, a, 14, year, old, f...  \n",
       "3  [Request, A, Free, No, Obligation, Consultatio...  \n",
       "4  [Is, there, a, way, to, look, for, a, particul...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('C:/Users/osval/Documents/Clasificacion_Texto/datasets/email/csv/spam-apache.csv', names = ['clase','contenido'])\n",
    "df['tokens'] = df['contenido'].apply(lambda x: word_tokenize(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "OvHkYDylNMKP"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<',\n",
       " '!',\n",
       " 'DOCTYPE',\n",
       " 'HTML',\n",
       " 'PUBLIC',\n",
       " '``',\n",
       " '-//W3C//DTD',\n",
       " 'HTML',\n",
       " '4.0',\n",
       " 'Transitional//EN',\n",
       " \"''\",\n",
       " '>',\n",
       " '<',\n",
       " 'HTML',\n",
       " '>',\n",
       " '<',\n",
       " 'HEAD',\n",
       " '>',\n",
       " '<',\n",
       " 'META',\n",
       " 'http-equiv=Content-Type',\n",
       " 'content=',\n",
       " \"''\",\n",
       " 'text/html',\n",
       " ';',\n",
       " 'charset=iso-8859-1',\n",
       " \"''\",\n",
       " '>',\n",
       " '<',\n",
       " 'META',\n",
       " 'content=',\n",
       " \"''\",\n",
       " 'MSHTML',\n",
       " '6.00.2600.0',\n",
       " \"''\",\n",
       " 'name=GENERATOR',\n",
       " '>',\n",
       " '<',\n",
       " 'STYLE',\n",
       " '>',\n",
       " '<',\n",
       " '/STYLE',\n",
       " '>',\n",
       " '<',\n",
       " '/HEAD',\n",
       " '>',\n",
       " '<',\n",
       " 'BODY',\n",
       " 'bgColor=',\n",
       " '#',\n",
       " 'ffffff',\n",
       " '>',\n",
       " '<',\n",
       " 'DIV',\n",
       " '>',\n",
       " '<',\n",
       " 'FONT',\n",
       " 'face=Arial',\n",
       " 'size=2',\n",
       " '>',\n",
       " '<',\n",
       " 'FONT',\n",
       " 'face=',\n",
       " \"''\",\n",
       " 'Times',\n",
       " 'New',\n",
       " 'Roman',\n",
       " \"''\",\n",
       " 'size=3',\n",
       " '>',\n",
       " 'Dear',\n",
       " 'Friend',\n",
       " ',',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'A',\n",
       " 'recent',\n",
       " 'survey',\n",
       " 'by',\n",
       " 'Nielsen/Netratings',\n",
       " 'says',\n",
       " 'that',\n",
       " '``',\n",
       " 'The',\n",
       " 'Internet',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'population',\n",
       " 'is',\n",
       " 'rapidly',\n",
       " 'approaching',\n",
       " 'a',\n",
       " \"'Half\",\n",
       " 'a',\n",
       " 'Billion',\n",
       " \"'\",\n",
       " 'people',\n",
       " '!',\n",
       " '``',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'SO',\n",
       " 'WHAT',\n",
       " 'DOES',\n",
       " 'ALL',\n",
       " 'THIS',\n",
       " 'MEAN',\n",
       " 'TO',\n",
       " 'YOU',\n",
       " '?',\n",
       " 'EASY',\n",
       " 'MONEY',\n",
       " '!',\n",
       " '!',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'Let',\n",
       " \"'s\",\n",
       " 'assume',\n",
       " 'that',\n",
       " 'every',\n",
       " 'person',\n",
       " 'has',\n",
       " 'only',\n",
       " 'one',\n",
       " 'E-mail',\n",
       " 'address',\n",
       " '...',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'that',\n",
       " \"'s\",\n",
       " '500',\n",
       " 'million',\n",
       " 'potential',\n",
       " 'customers',\n",
       " 'and',\n",
       " 'growing',\n",
       " '!',\n",
       " 'In',\n",
       " 'addition',\n",
       " ',',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " \"E'mail\",\n",
       " 'is',\n",
       " 'without',\n",
       " 'question',\n",
       " 'the',\n",
       " 'most',\n",
       " 'powerful',\n",
       " 'method',\n",
       " 'of',\n",
       " 'distributing',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'information',\n",
       " 'on',\n",
       " 'the',\n",
       " 'face',\n",
       " 'of',\n",
       " 'the',\n",
       " 'earth.',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'Well',\n",
       " ',',\n",
       " 'I',\n",
       " 'think',\n",
       " 'you',\n",
       " 'get',\n",
       " 'the',\n",
       " 'picture',\n",
       " '.',\n",
       " 'The',\n",
       " 'numbers',\n",
       " 'and',\n",
       " 'potential',\n",
       " 'are',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'just',\n",
       " 'staggering',\n",
       " ',',\n",
       " 'but',\n",
       " 'it',\n",
       " 'gets',\n",
       " 'even',\n",
       " 'better',\n",
       " '...',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'Suppose',\n",
       " 'I',\n",
       " 'told',\n",
       " 'you',\n",
       " 'that',\n",
       " 'you',\n",
       " 'could',\n",
       " 'start',\n",
       " 'your',\n",
       " 'own',\n",
       " 'E-mail',\n",
       " 'business',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'today',\n",
       " 'and',\n",
       " 'enjoy',\n",
       " 'these',\n",
       " 'benefits',\n",
       " ':',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '*',\n",
       " '*',\n",
       " '*',\n",
       " 'All',\n",
       " 'Customers',\n",
       " 'Pay',\n",
       " 'You',\n",
       " 'In',\n",
       " 'Cash',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '*',\n",
       " '*',\n",
       " '*',\n",
       " 'You',\n",
       " 'Will',\n",
       " 'Sell',\n",
       " 'A',\n",
       " 'Product',\n",
       " 'Which',\n",
       " 'Costs',\n",
       " 'Nothing',\n",
       " 'to',\n",
       " 'Produce',\n",
       " '!',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '*',\n",
       " '*',\n",
       " '*',\n",
       " 'Your',\n",
       " 'Only',\n",
       " 'Overhead',\n",
       " 'Is',\n",
       " 'Your',\n",
       " 'Time',\n",
       " '!',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '*',\n",
       " '*',\n",
       " '*',\n",
       " 'You',\n",
       " 'Have',\n",
       " '100s',\n",
       " 'Of',\n",
       " 'Millions',\n",
       " 'Of',\n",
       " 'Potential',\n",
       " 'Customers',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '*',\n",
       " '*',\n",
       " '*',\n",
       " 'You',\n",
       " 'Get',\n",
       " 'Detailed',\n",
       " ',',\n",
       " 'Easy',\n",
       " 'To',\n",
       " 'Follow',\n",
       " 'Startup',\n",
       " 'Instructions',\n",
       " '!',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'AND',\n",
       " 'THIS',\n",
       " 'IS',\n",
       " 'JUST',\n",
       " 'THE',\n",
       " 'TIP',\n",
       " 'OF',\n",
       " 'THE',\n",
       " 'ICEBERG',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'As',\n",
       " 'you',\n",
       " 'read',\n",
       " 'on',\n",
       " 'you',\n",
       " \"'ll\",\n",
       " 'discover',\n",
       " 'how',\n",
       " 'a',\n",
       " \"'Seen\",\n",
       " 'on',\n",
       " 'National',\n",
       " 'TV',\n",
       " \"'\",\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'program',\n",
       " 'is',\n",
       " 'paying',\n",
       " 'out',\n",
       " 'a',\n",
       " 'half',\n",
       " 'million',\n",
       " 'dollars',\n",
       " ',',\n",
       " 'every',\n",
       " '4',\n",
       " 'to',\n",
       " '5',\n",
       " 'months',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'from',\n",
       " 'your',\n",
       " 'home',\n",
       " ',',\n",
       " 'for',\n",
       " 'an',\n",
       " 'investment',\n",
       " 'of',\n",
       " 'only',\n",
       " '$',\n",
       " '25',\n",
       " 'US',\n",
       " 'Dollars',\n",
       " 'expense',\n",
       " ',',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'one',\n",
       " 'time',\n",
       " '.',\n",
       " 'ALL',\n",
       " 'THANKS',\n",
       " 'TO',\n",
       " 'THE',\n",
       " 'COMPUTER',\n",
       " 'AGE',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'AND',\n",
       " 'THE',\n",
       " 'INTERNET',\n",
       " '!',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'Before',\n",
       " 'you',\n",
       " 'say',\n",
       " '``',\n",
       " 'Bull',\n",
       " \"''\",\n",
       " ',',\n",
       " 'please',\n",
       " 'read',\n",
       " 'the',\n",
       " 'following',\n",
       " ':',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'This',\n",
       " 'is',\n",
       " 'the',\n",
       " 'letter',\n",
       " 'you',\n",
       " 'have',\n",
       " 'been',\n",
       " 'hearing',\n",
       " 'about',\n",
       " 'on',\n",
       " 'the',\n",
       " 'news',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'lately',\n",
       " '.',\n",
       " 'Due',\n",
       " 'to',\n",
       " 'the',\n",
       " 'popularity',\n",
       " 'of',\n",
       " 'this',\n",
       " 'letter',\n",
       " 'on',\n",
       " 'the',\n",
       " 'Internet',\n",
       " ',',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'a',\n",
       " 'national',\n",
       " 'weekly',\n",
       " 'news',\n",
       " 'program',\n",
       " 'recently',\n",
       " 'devoted',\n",
       " 'an',\n",
       " 'entire',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'show',\n",
       " 'to',\n",
       " 'the',\n",
       " 'investigation',\n",
       " 'of',\n",
       " 'this',\n",
       " 'program',\n",
       " 'described',\n",
       " 'below',\n",
       " ',',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'to',\n",
       " 'see',\n",
       " 'if',\n",
       " 'it',\n",
       " 'really',\n",
       " 'can',\n",
       " 'make',\n",
       " 'people',\n",
       " 'money.',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'The',\n",
       " 'show',\n",
       " 'also',\n",
       " 'investigated',\n",
       " 'whether',\n",
       " 'or',\n",
       " 'not',\n",
       " 'the',\n",
       " 'program',\n",
       " 'was',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'legal',\n",
       " '.',\n",
       " 'Their',\n",
       " 'findings',\n",
       " 'proved',\n",
       " 'once',\n",
       " 'and',\n",
       " 'for',\n",
       " 'all',\n",
       " 'that',\n",
       " 'there',\n",
       " 'are',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " \"''\",\n",
       " 'absolutely',\n",
       " 'NO',\n",
       " 'laws',\n",
       " 'prohibiting',\n",
       " 'the',\n",
       " 'participation',\n",
       " 'in',\n",
       " 'the',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'program',\n",
       " 'and',\n",
       " 'if',\n",
       " 'people',\n",
       " 'can',\n",
       " 'follow',\n",
       " 'the',\n",
       " 'simple',\n",
       " 'instructions',\n",
       " ',',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'they',\n",
       " 'are',\n",
       " 'bound',\n",
       " 'to',\n",
       " 'make',\n",
       " 'some',\n",
       " 'mega',\n",
       " 'bucks',\n",
       " 'with',\n",
       " 'only',\n",
       " '$',\n",
       " '25',\n",
       " 'out',\n",
       " 'of',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'pocket',\n",
       " 'cost',\n",
       " \"''\",\n",
       " '.',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'DUE',\n",
       " 'TO',\n",
       " 'THE',\n",
       " 'RECENT',\n",
       " 'INCREASE',\n",
       " 'OF',\n",
       " 'POPULARITY',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'AND',\n",
       " 'RESPECT',\n",
       " 'THIS',\n",
       " 'PROGRAM',\n",
       " 'HAS',\n",
       " 'ATTAINED',\n",
       " ',',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'IT',\n",
       " 'IS',\n",
       " 'CURRENTLY',\n",
       " 'WORKING',\n",
       " 'BETTER',\n",
       " 'THAN',\n",
       " 'EVER',\n",
       " '!',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '*',\n",
       " '*',\n",
       " '*',\n",
       " '*',\n",
       " '*',\n",
       " 'This',\n",
       " 'is',\n",
       " 'what',\n",
       " 'one',\n",
       " 'had',\n",
       " 'to',\n",
       " 'say',\n",
       " ':',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " \"''\",\n",
       " 'Thanks',\n",
       " 'to',\n",
       " 'this',\n",
       " 'profitable',\n",
       " 'opportunity',\n",
       " '.',\n",
       " 'I',\n",
       " 'was',\n",
       " 'approached',\n",
       " 'many',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'times',\n",
       " 'before',\n",
       " 'but',\n",
       " 'each',\n",
       " 'time',\n",
       " 'I',\n",
       " 'passed',\n",
       " 'on',\n",
       " 'it',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'so',\n",
       " 'glad',\n",
       " 'I',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'finally',\n",
       " 'joined',\n",
       " 'just',\n",
       " 'to',\n",
       " 'see',\n",
       " 'what',\n",
       " 'one',\n",
       " 'could',\n",
       " 'expect',\n",
       " 'in',\n",
       " 'return',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'for',\n",
       " 'the',\n",
       " 'minimal',\n",
       " 'effort',\n",
       " 'and',\n",
       " 'money',\n",
       " 'required',\n",
       " '.',\n",
       " 'To',\n",
       " 'my',\n",
       " 'asonishment',\n",
       " ',',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'I',\n",
       " 'received',\n",
       " 'total',\n",
       " '$',\n",
       " '610,470.00',\n",
       " 'in',\n",
       " '21',\n",
       " 'weeks',\n",
       " ',',\n",
       " 'with',\n",
       " 'money',\n",
       " 'still',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'coming',\n",
       " 'in',\n",
       " \"''\",\n",
       " '.',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'Pam',\n",
       " 'Hedland',\n",
       " ',',\n",
       " 'Fort',\n",
       " 'Lee',\n",
       " ',',\n",
       " 'New',\n",
       " 'Jersey',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'Here',\n",
       " 'is',\n",
       " 'another',\n",
       " 'testimonial',\n",
       " ':',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " \"''\",\n",
       " 'This',\n",
       " 'program',\n",
       " 'has',\n",
       " 'been',\n",
       " 'around',\n",
       " 'for',\n",
       " 'a',\n",
       " 'long',\n",
       " 'time',\n",
       " 'but',\n",
       " 'I',\n",
       " 'never',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'believed',\n",
       " 'in',\n",
       " 'it',\n",
       " '.',\n",
       " 'But',\n",
       " 'one',\n",
       " 'day',\n",
       " 'when',\n",
       " 'I',\n",
       " 'received',\n",
       " 'this',\n",
       " 'again',\n",
       " 'in',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'the',\n",
       " 'mail',\n",
       " 'I',\n",
       " 'decided',\n",
       " 'to',\n",
       " 'gamble',\n",
       " 'my',\n",
       " '$',\n",
       " '25',\n",
       " 'on',\n",
       " 'it',\n",
       " '.',\n",
       " 'I',\n",
       " 'followed',\n",
       " 'the',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'simple',\n",
       " 'instructions',\n",
       " 'and',\n",
       " 'walaa',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '3',\n",
       " 'weeks',\n",
       " 'later',\n",
       " 'the',\n",
       " 'money',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'started',\n",
       " 'to',\n",
       " 'come',\n",
       " 'in',\n",
       " '.',\n",
       " 'First',\n",
       " 'month',\n",
       " 'I',\n",
       " 'only',\n",
       " 'made',\n",
       " '$',\n",
       " '240.00',\n",
       " 'but',\n",
       " 'the',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'next',\n",
       " '2',\n",
       " 'months',\n",
       " 'after',\n",
       " 'that',\n",
       " 'I',\n",
       " 'made',\n",
       " 'a',\n",
       " 'total',\n",
       " 'of',\n",
       " '$',\n",
       " '290,000.00.',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'So',\n",
       " 'far',\n",
       " ',',\n",
       " 'in',\n",
       " 'the',\n",
       " 'past',\n",
       " '8',\n",
       " 'months',\n",
       " 'by',\n",
       " 're-entering',\n",
       " 'the',\n",
       " 'program',\n",
       " ',',\n",
       " 'I',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'have',\n",
       " 'made',\n",
       " 'over',\n",
       " '$',\n",
       " '710,000.00',\n",
       " 'and',\n",
       " 'I',\n",
       " 'am',\n",
       " 'playing',\n",
       " 'it',\n",
       " 'again.',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'The',\n",
       " 'key',\n",
       " 'to',\n",
       " 'success',\n",
       " 'in',\n",
       " 'this',\n",
       " 'program',\n",
       " 'is',\n",
       " 'to',\n",
       " 'follow',\n",
       " 'the',\n",
       " 'simple',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'steps',\n",
       " 'and',\n",
       " 'NOT',\n",
       " 'change',\n",
       " 'anything',\n",
       " '.',\n",
       " '``',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'More',\n",
       " 'testimonials',\n",
       " 'later',\n",
       " 'but',\n",
       " 'first',\n",
       " ':',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '*',\n",
       " '*',\n",
       " 'PRINT',\n",
       " 'THIS',\n",
       " 'NOW',\n",
       " 'FOR',\n",
       " 'YOUR',\n",
       " 'FUTURE',\n",
       " 'REFERENCE',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'If',\n",
       " 'you',\n",
       " 'would',\n",
       " 'like',\n",
       " 'to',\n",
       " 'make',\n",
       " 'at',\n",
       " 'least',\n",
       " '$',\n",
       " '500,000',\n",
       " 'every',\n",
       " '4',\n",
       " 'to',\n",
       " '5',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'months',\n",
       " 'easily',\n",
       " 'and',\n",
       " 'comfortably',\n",
       " ',',\n",
       " 'please',\n",
       " 'read',\n",
       " 'the',\n",
       " 'following',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'THEN',\n",
       " 'READ',\n",
       " ...]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokens'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "O4kw1BQUOe4-"
   },
   "outputs": [],
   "source": [
    "all_words = nltk.FreqDist([w for tokenlist in df['tokens'].values for w in tokenlist])\n",
    "top_words = all_words.most_common(200)\n",
    "\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in top_words:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "1g6F_qNfmRAW"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"contains((',', 2173))\": False,\n",
       " \"contains(('.', 2171))\": False,\n",
       " \"contains(('the', 1967))\": False,\n",
       " \"contains(('>', 1787))\": False,\n",
       " \"contains(('--', 1611))\": False,\n",
       " \"contains(('to', 1435))\": False,\n",
       " \"contains((':', 1220))\": False,\n",
       " \"contains(('*', 1149))\": False,\n",
       " \"contains(('and', 1064))\": False,\n",
       " \"contains(('of', 958))\": False,\n",
       " \"contains(('a', 879))\": False,\n",
       " \"contains(('you', 744))\": False,\n",
       " \"contains(('in', 742))\": False,\n",
       " \"contains(('I', 741))\": False,\n",
       " \"contains(('<', 718))\": False,\n",
       " \"contains(('!', 698))\": False,\n",
       " \"contains(('%', 677))\": False,\n",
       " \"contains(('for', 609))\": False,\n",
       " \"contains(('is', 578))\": False,\n",
       " \"contains(('#', 521))\": False,\n",
       " \"contains(('BR', 494))\": False,\n",
       " \"contains(('that', 479))\": False,\n",
       " \"contains((')', 463))\": False,\n",
       " \"contains(('it', 458))\": False,\n",
       " 'contains((\"\\'\\'\", 434))': False,\n",
       " \"contains(('$', 413))\": False,\n",
       " \"contains(('this', 384))\": False,\n",
       " \"contains(('(', 380))\": False,\n",
       " \"contains(('on', 378))\": False,\n",
       " \"contains(('http', 362))\": False,\n",
       " \"contains(('?', 360))\": False,\n",
       " \"contains(('your', 359))\": False,\n",
       " \"contains(('have', 351))\": False,\n",
       " \"contains(('with', 334))\": False,\n",
       " \"contains(('``', 307))\": False,\n",
       " \"contains(('be', 299))\": False,\n",
       " \"contains(('-', 289))\": False,\n",
       " \"contains(('from', 271))\": False,\n",
       " 'contains((\"\\'s\", 263))': False,\n",
       " \"contains(('are', 257))\": False,\n",
       " \"contains(('31', 255))\": False,\n",
       " \"contains(('or', 252))\": False,\n",
       " \"contains(('as', 251))\": False,\n",
       " \"contains(('will', 243))\": False,\n",
       " \"contains(('not', 226))\": False,\n",
       " \"contains(('30', 220))\": False,\n",
       " \"contains(('my', 206))\": False,\n",
       " \"contains(('at', 199))\": False,\n",
       " \"contains(('The', 198))\": False,\n",
       " \"contains(('has', 195))\": False,\n",
       " \"contains(('can', 194))\": False,\n",
       " \"contains(('&', 181))\": False,\n",
       " \"contains(('all', 176))\": False,\n",
       " 'contains((\"n\\'t\", 175))': False,\n",
       " \"contains(('do', 167))\": False,\n",
       " \"contains(('out', 166))\": False,\n",
       " \"contains(('but', 164))\": False,\n",
       " \"contains(('...', 160))\": False,\n",
       " \"contains(('our', 160))\": False,\n",
       " \"contains(('by', 156))\": False,\n",
       " \"contains(('if', 152))\": False,\n",
       " \"contains(('was', 149))\": False,\n",
       " \"contains(('one', 129))\": False,\n",
       " \"contains(('an', 129))\": False,\n",
       " \"contains(('just', 128))\": False,\n",
       " \"contains(('@', 128))\": False,\n",
       " \"contains(('This', 125))\": False,\n",
       " \"contains(('1', 123))\": False,\n",
       " \"contains(('If', 118))\": False,\n",
       " \"contains(('more', 118))\": False,\n",
       " \"contains(('You', 117))\": False,\n",
       " \"contains(('5', 116))\": False,\n",
       " \"contains(('we', 116))\": False,\n",
       " \"contains(('time', 114))\": False,\n",
       " \"contains(('people', 110))\": False,\n",
       " \"contains(('me', 110))\": False,\n",
       " \"contains(('We', 110))\": False,\n",
       " \"contains(('THE', 108))\": False,\n",
       " \"contains(('up', 108))\": False,\n",
       " \"contains(('get', 107))\": False,\n",
       " \"contains(('they', 103))\": False,\n",
       " \"contains(('only', 100))\": False,\n",
       " \"contains(('like', 100))\": False,\n",
       " \"contains(('so', 99))\": False,\n",
       " 'contains((\"\\'\", 95))': False,\n",
       " \"contains(('To', 95))\": False,\n",
       " \"contains(('list', 95))\": False,\n",
       " \"contains(('2', 94))\": False,\n",
       " \"contains(('other', 92))\": False,\n",
       " \"contains(('A', 91))\": False,\n",
       " \"contains(('FREE', 90))\": False,\n",
       " \"contains(('No', 90))\": False,\n",
       " \"contains(('would', 88))\": False,\n",
       " \"contains(('any', 88))\": False,\n",
       " \"contains(('been', 87))\": False,\n",
       " \"contains(('who', 87))\": False,\n",
       " \"contains(('there', 86))\": False,\n",
       " \"contains(('which', 86))\": False,\n",
       " \"contains(('|', 84))\": False,\n",
       " \"contains(('about', 81))\": False,\n",
       " \"contains((']', 81))\": False,\n",
       " \"contains(('some', 80))\": False,\n",
       " \"contains(('email', 80))\": False,\n",
       " \"contains(('what', 79))\": False,\n",
       " \"contains(('AND', 77))\": False,\n",
       " \"contains(('their', 76))\": False,\n",
       " \"contains(('TO', 75))\": False,\n",
       " \"contains(('no', 75))\": False,\n",
       " \"contains(('then', 74))\": False,\n",
       " \"contains(('his', 74))\": False,\n",
       " \"contains(('It', 74))\": False,\n",
       " \"contains(('address', 73))\": False,\n",
       " \"contains(('use', 73))\": False,\n",
       " \"contains(('YOU', 72))\": False,\n",
       " \"contains(('money', 72))\": False,\n",
       " \"contains(('3', 72))\": False,\n",
       " \"contains(('[', 71))\": False,\n",
       " \"contains(('each', 70))\": False,\n",
       " \"contains(('work', 70))\": False,\n",
       " \"contains(('over', 69))\": False,\n",
       " \"contains(('he', 69))\": False,\n",
       " \"contains(('make', 68))\": False,\n",
       " \"contains(('send', 68))\": False,\n",
       " \"contains(('them', 68))\": False,\n",
       " \"contains(('OF', 67))\": False,\n",
       " \"contains(('name', 67))\": False,\n",
       " \"contains(('than', 67))\": False,\n",
       " \"contains(('2002', 67))\": False,\n",
       " \"contains(('could', 66))\": False,\n",
       " \"contains(('am', 66))\": False,\n",
       " \"contains(('unseen', 65))\": False,\n",
       " \"contains(('see', 63))\": False,\n",
       " \"contains(('YOUR', 63))\": False,\n",
       " \"contains(('4', 61))\": False,\n",
       " \"contains(('how', 60))\": False,\n",
       " \"contains(('way', 60))\": False,\n",
       " \"contains(('msgs', 59))\": False,\n",
       " \"contains(('lists/l-k', 59))\": False,\n",
       " \"contains(('wrote', 58))\": False,\n",
       " \"contains(('also', 57))\": False,\n",
       " \"contains(('here', 57))\": False,\n",
       " \"contains(('Your', 56))\": False,\n",
       " \"contains(('mail', 56))\": False,\n",
       " \"contains(('receive', 56))\": False,\n",
       " \"contains(('go', 56))\": False,\n",
       " \"contains(('program', 55))\": False,\n",
       " \"contains(('On', 55))\": False,\n",
       " \"contains(('new', 55))\": False,\n",
       " \"contains(('had', 54))\": False,\n",
       " \"contains(('NOT', 54))\": False,\n",
       " \"contains(('does', 54))\": False,\n",
       " \"contains(('want', 54))\": False,\n",
       " \"contains(('please', 53))\": False,\n",
       " \"contains(('us', 53))\": False,\n",
       " \"contains(('because', 53))\": False,\n",
       " \"contains(('REPORT', 52))\": False,\n",
       " \"contains(('below', 51))\": False,\n",
       " \"contains(('when', 51))\": False,\n",
       " \"contains(('e-mail', 51))\": False,\n",
       " 'contains((\"\\'m\", 51))': False,\n",
       " \"contains(('free', 50))\": False,\n",
       " \"contains(('think', 49))\": False,\n",
       " \"contains(('now', 49))\": False,\n",
       " \"contains(('first', 48))\": False,\n",
       " \"contains(('Please', 48))\": False,\n",
       " \"contains(('most', 47))\": False,\n",
       " \"contains(('within', 47))\": False,\n",
       " \"contains(('even', 46))\": False,\n",
       " \"contains(('using', 46))\": False,\n",
       " \"contains(('e-mails', 45))\": False,\n",
       " \"contains(('sent', 45))\": False,\n",
       " \"contains(('THIS', 44))\": False,\n",
       " \"contains(('In', 44))\": False,\n",
       " \"contains(('received', 44))\": False,\n",
       " \"contains(('company', 44))\": False,\n",
       " \"contains(('need', 43))\": False,\n",
       " \"contains(('much', 43))\": False,\n",
       " \"contains(('nbsp=3B', 43))\": False,\n",
       " \"contains(('did', 42))\": False,\n",
       " \"contains(('And', 42))\": False,\n",
       " \"contains(('still', 41))\": False,\n",
       " \"contains(('FOR', 41))\": False,\n",
       " 'contains((\"\\'re\", 41))': False,\n",
       " \"contains(('know', 41))\": False,\n",
       " \"contains((';', 40))\": False,\n",
       " \"contains(('=', 40))\": False,\n",
       " \"contains(('days', 40))\": False,\n",
       " \"contains(('where', 40))\": False,\n",
       " \"contains(('//www.adclick.ws/p.cfm', 40))\": False,\n",
       " \"contains(('line', 40))\": False,\n",
       " \"contains(('information', 39))\": False,\n",
       " \"contains(('US', 39))\": False,\n",
       " \"contains(('through', 39))\": False,\n",
       " \"contains(('message', 39))\": False,\n",
       " \"contains(('Linux', 39))\": False,\n",
       " 'contains((\"\\'ve\", 39))': False,\n",
       " \"contains(('made', 38))\": False,\n",
       " \"contains(('different', 38))\": False,\n",
       " \"contains(('those', 38))\": False,\n",
       " \"contains(('Report', 38))\": False}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_features(df['tokens'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "SrCXXGMCn3zz"
   },
   "outputs": [],
   "source": [
    "fset = [(document_features(texto), clase) for texto, clase in zip(df['tokens'].values, df['clase'].values)]\n",
    "random.shuffle(fset)\n",
    "train, test = fset[:200], fset[200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "r6FGZE4OqkEa"
   },
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 32
    },
    "id": "xIyVc6lBrGOy",
    "outputId": "01479a1e-6f68-4423-d4d3-6b1f9673f43c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(classifier, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 115
    },
    "id": "1x-R_PImrKIV",
    "outputId": "3b384c67-1640-422f-d9a1-19e41b42c667"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "     contains((\"'\", 95)) = False              -1 : 1      =      1.0 : 1.0\n",
      "   contains((\"''\", 434)) = False              -1 : 1      =      1.0 : 1.0\n",
      "    contains((\"'m\", 51)) = False              -1 : 1      =      1.0 : 1.0\n",
      "   contains((\"'re\", 41)) = False              -1 : 1      =      1.0 : 1.0\n",
      "   contains((\"'s\", 263)) = False              -1 : 1      =      1.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "id": "wKzgha92up3l",
    "outputId": "ec079b59-5973-4084-e6ab-2f66037e92e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      <!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Tr...\n",
       "2      Help wanted.  We are a 14 year old fortune 500...\n",
       "3      Request A Free No Obligation Consultation!\\r\\n...\n",
       "10     >\\r\\n>“µ×è¹µÑÇ ¡ÑºâÅ¡¸ØÃ¡Ô¨º¹ÍÔ¹àµÍÃìà¹çµ” \\r\\...\n",
       "11     ==============================================...\n",
       "                             ...                        \n",
       "243    ##############################################...\n",
       "244    Wanna see sexually curious teens playing with ...\n",
       "246    REQUEST FOR URGENT BUSINESS ASSISTANCE\\r\\n----...\n",
       "248    Email marketing works!  There's no way around ...\n",
       "249    Email marketing works!  There's no way around ...\n",
       "Name: contenido, Length: 125, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['clase']==-1]['contenido']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YeBvifrnr3GY"
   },
   "source": [
    "## Ejercicio de práctica\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AR53vedlvd1O"
   },
   "source": [
    "¿Como podrías construir un mejor clasificador de documentos?\n",
    "\n",
    "0. **Dataset más grande:** El conjunto de datos que usamos fue muy pequeño, considera usar los archivos corpus que estan ubicados en la ruta: `datasets/email/plaintext/` \n",
    "\n",
    "1. **Limpieza:** como te diste cuenta no hicimos ningun tipo de limpieza de texto en los correos electrónicos. Considera usar expresiones regulares, filtros por categorias gramaticales, etc ... . \n",
    "\n",
    "---\n",
    "\n",
    "Con base en eso construye un dataset más grande y con un tokenizado más pulido. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "TOw2KrtnymVT"
   },
   "outputs": [],
   "source": [
    "# Descomprimir ZIP\n",
    "import zipfile\n",
    "fantasy_zip = zipfile.ZipFile('C:/Users/osval/Documents/Clasificacion_Texto/datasets/email/plaintext/corpus1.zip')\n",
    "fantasy_zip.extractall('C:/Users/osval/Documents/Clasificacion_Texto/datasets/email/plaintext')\n",
    "fantasy_zip.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "v2ZO0aJyrTLx"
   },
   "outputs": [],
   "source": [
    "# Creamos un listado de los archivos dentro del Corpus1 ham/spam\n",
    "from os import listdir\n",
    "\n",
    "path_ham = \"C:/Users/osval/Documents/Clasificacion_Texto/datasets/email/plaintext/corpus1/ham/\"\n",
    "filepaths_ham = [path_ham+f for f in listdir(path_ham) if f.endswith('.txt')]\n",
    "\n",
    "path_spam = \"C:/Users/osval/Documents/Clasificacion_Texto/datasets/email/plaintext/corpus1/spam/\"\n",
    "filepaths_spam = [path_spam+f for f in listdir(path_spam) if f.endswith('.txt')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos la funcion para tokenizar y leer los archivos \n",
    "\n",
    "def abrir(texto):\n",
    "  with open(texto, 'r', errors='ignore') as f2:\n",
    "    data = f2.read()\n",
    "    data = word_tokenize(data)\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos la lista tokenizada del ham\n",
    "list_ham = list(map(abrir, filepaths_ham))\n",
    "# Creamos la lista tokenizada del spam\n",
    "list_spam = list(map(abrir, filepaths_spam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\osval\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separamos las palabras mas comunes\n",
    "all_words = nltk.FreqDist([w for tokenlist in list_ham+list_spam for w in tokenlist])\n",
    "top_words = all_words.most_common(250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Agregamos Bigramas\n",
    "bigram_text = nltk.Text([w for token in list_ham+list_spam for w in token])\n",
    "bigrams = list(nltk.bigrams(bigram_text))\n",
    "top_bigrams = (nltk.FreqDist(bigrams)).most_common(250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    bigram = set(list(nltk.bigrams(nltk.Text([token for token in document]))))\n",
    "    features = {}\n",
    "    for word, j in top_words:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "\n",
    "    for bigrams, i in top_bigrams:\n",
    "        features['contains_bigram({})'.format(bigrams)] = (bigrams in bigram)\n",
    "  \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9V_KmDBHwiy8"
   },
   "source": [
    "2. **Validación del modelo anterior:**  \n",
    "---\n",
    "\n",
    "una vez tengas el nuevo conjunto de datos más pulido y de mayor tamaño, considera el mismo entrenamiento con el mismo tipo de atributos del ejemplo anterior, ¿mejora el accuracy del modelo resultante?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "AM6Vhy-Fw8oj"
   },
   "outputs": [],
   "source": [
    "# Juntamos las listas indicando si tienen palabras de las mas comunes\n",
    "import random\n",
    "fset_ham = [(document_features(texto), 0) for texto in list_ham]\n",
    "fset_spam = [(document_features(texto), 1) for texto in list_spam]\n",
    "fset = fset_spam + fset_ham[:1500]\n",
    "random.shuffle(fset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separamos en las listas en train y test\n",
    "from sklearn.model_selection import train_test_split\n",
    "fset_train, fset_test = train_test_split(fset, test_size=0.20, random_state=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos el programa\n",
    "classifier = nltk.NaiveBayesClassifier.train(fset_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2lC72_CbxAoJ"
   },
   "source": [
    "3. **Construye mejores atributos**: A veces no solo se trata de las palabras más frecuentes sino de el contexto, y capturar contexto no es posible solo viendo los tokens de forma individual, ¿que tal si consideramos bi-gramas, tri-gramas ...?, ¿las secuencias de palabras podrián funcionar como mejores atributos para el modelo?. Para ver si es así,  podemos extraer n-gramas de nuestro corpus y obtener sus frecuencias de aparición con `FreqDist()`, desarrolla tu propia manera de hacerlo y entrena un modelo con esos nuevos atributos, no olvides compartir tus resultados en la sección de comentarios. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "wtMkQWpfxoy3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8666666666666667\n"
     ]
    }
   ],
   "source": [
    "# Probamos y calificamos\n",
    "classifier.classify(document_features(list_ham[34]))\n",
    "print(nltk.classify.accuracy(classifier, fset_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "[Lecture_19/20]Modelos_clasificacion.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
